
#  Artificial Neural Networks – Spring 2025 🎓

Welcome to the GitHub repository for the **Artificial Neural Networks** course at FAST NUCES, Spring 2025.  
This repo contains notes, assignments, notebooks, and lecture-related material.

📺 **YouTube Playlist** :  
[Watch all lectures here](https://youtube.com/playlist?list=PLHvklJ3ab98YvLHVtDwOng9s4Xf5uNC-c&si=v2K5VLbdiCyDpqjZ)

---

## 🧠 Topics Covered

Below is a comprehensive list of all **topics** covered in this course:

### 📊 Regression & Optimization
- Linear Regression  
- Multi Linear Regression  
- Polynomial Regression  
- Gradient Descent & its Variants  
- Learning Pipelines  

### 🧮 Classification & Loss Functions
- Logistic Regression (Binary Classification)  
  - Linear vs Non-Linear Models  
- Loss Functions:  
  - Mean Squared Error (MSE)  
  - Cross Entropy Loss  

### 🧠 Artificial Neural Networks (ANN)
- Perceptrons  
- Neural Networks  
- Feed Forward Networks  
- Backpropagation  
- Decision Boundaries  
- Binary Cross Entropy  
- Activation Functions:  
  - Sigmoid  
  - Tanh  
  - ReLU  
  - Softmax  

### 🔍 Deep & Sparse Neural Networks
- Deep Neural Networks  
- Sparse Neural Networks  
- Vanishing/Exploding Gradient Problem  
- Residual Neural Networks (ResNet)  
- Batch Normalization  
- Layer Normalization  

### 🧬 Advanced Neural Models
- Hopfield Neural Networks  


### 🖼️ Convolutional Neural Networks (CNNs)
- Convolution Operation  
- Pooling  
- Padding  
- Stride  
- Time Series Input into CNNs  

### 📏 Model Evaluation Metrics
- Confusion Matrix  
- Model Accuracy  
- Precision, Recall, F1 Score  
- True Positive Rate (TPR)  
- False Positive Rate (FPR)  

### 🔁 Recurrent Neural Networks (RNNs)
- RNN Architecture & Applications  

### 🔄 Sequence Models & Transformers
- Seq2Seq Models  
- Attention Mechanisms:  
  - Self-Attention  
  - Multi-Head Attention  
  - Masking  
  - Cross-Attention  
- Positional Encoding  
- Encoder / Decoder Architectures  
- Transformers  

---

## 📌 Note

This repository is maintained by a student for academic purposes.  
Feel free to **fork**, **star**, and **contribute** if you're learning or revising DL!

