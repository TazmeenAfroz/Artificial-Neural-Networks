{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "CNN's are a type of deep neural network architecture designed specifically for large matrices (specifically images). The fundamental operation inside a CNN are that of convolution, max pooling, and so on. In these sections, we will look at them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "Convolution is an important multiplication operation used videly in signal processing, image processing, machine learning, and other areas. The multiplication between two arrays $f$ and $g$ would be expressed as:\n",
    "\n",
    "$\\displaystyle C(i) = \\sum_j f(j) g(i - j)$\n",
    "\n",
    "Usually, $f$ is considered as an input function (in our case, an image), and $g$ is considered as a gaussian filter or kernel. Both $i$ and $j$ are index points to move across the arrays. There are variations to this formula which manipulate the directions of index point movements. Some behave like forward convolution, others as backwards convolution. Examples are:\n",
    "\n",
    "$\\displaystyle C(i) = \\sum_j f(j) g(i + j)$\n",
    "\n",
    "$\\displaystyle C(i) = \\sum_j f(i - j) g(j)$\n",
    "\n",
    "and so on. This is so that there is alignment between $f$ and kernel $g$ while sliding across the input. Note that this means that there may be convolutions aligned to middle/center also. So, considering a case of $f = [1, 2, 3, 4, 5]$, and $g = [1, 0, -1]$. For $g$ to slide across $f$ values of $1, 2, 3$, followed by $2, 3, 4$, and lastly $3, 4, 5$, we would use:\n",
    "\n",
    "$\\displaystyle C(i) = \\sum_j f(i + j) g(j)$\n",
    "\n",
    "This wold give:\n",
    "\n",
    "$\\displaystyle C[0] = f[0+0] g[0] + f[0+1] g[1] + f[0+2] g[2] = f[0]g[0] + f[1]g[1] + f[2]g[2]$\n",
    "\n",
    "$\\displaystyle C[1] = f[1+0] g[0] + f[1+1] g[1] + f[1+2] g[2] = f[1]g[0] + f[2]g[1] + f[3]g[2]$\n",
    "\n",
    "$\\displaystyle C[2] = f[2+0] g[0] + f[2+1] g[1] + f[2+2] g[2] = f[2]g[0] + f[3]g[1] + f[4]g[2]$\n",
    "\n",
    "What the convolution captures in the positions $C[0]$, $C[1]$, and $C[2]$ are patterns, edges, textures, or specific features within the input $f$ which are amplified or highlighted on the basis of kernel $g$. \n",
    "\n",
    "## Padding\n",
    "\n",
    "Alternatively, if we use:\n",
    "\n",
    "$\\displaystyle C(i) = \\sum_j f(i - j) g(j)$\n",
    "\n",
    "We would have:\n",
    "\n",
    "$\\displaystyle C[0] = f[0-0] g[0] + f[0-1] g[1] + f[0-2] g[2] = f[0]g[0] + f[-1]g[1] + f[-2]g[2]$\n",
    "\n",
    "$\\displaystyle C[1] = f[1-0] g[0] + f[1-1] g[1] + f[1-2] g[2] = f[1]g[0] + f[0]g[1] + f[-1]g[2]$\n",
    "\n",
    "$\\displaystyle C[2] = f[2-0] g[0] + f[2-1] g[1] + f[2-2] g[2] = f[2]g[0] + f[1]g[1] + f[0]g[2]$\n",
    "\n",
    "Due to the negative indexing of $f[-1]$ and $f[-2]$, we would require to apply left side padding to become $f = [0, 0, 1, 2, 3, 4, 5]$. This would also mean that to span all the array element, we would additionally need:\n",
    "\n",
    "$\\displaystyle C[3] = f[3-0] g[0] + f[3-1] g[1] + f[3-2] g[2] = f[3]g[0] + f[2]g[1] + f[1]g[2]$\n",
    "\n",
    "$\\displaystyle C[4] = f[4-0] g[0] + f[4-1] g[1] + f[4-2] g[2] = f[4]g[0] + f[3]g[1] + f[2]g[2]$\n",
    "\n",
    "So, padding is simply the inclusion of some 0 values to the left $P_l$, right $P_r$, or both sides of an array $P$. In CNN, usually $P$ is considered. So $P = 2$ would mean that an image would be padded with two zeros in all 4 sides of an image. \n",
    "\n",
    "## Strides\n",
    "\n",
    "In the above cases, the index positions move by a distance of 1 across the array values. This is called 1-stride or $S = 1$. Occassionally, we would want it to move across 2 positions, or even larger positions. Why would we do it? To help capture local features, or global features. This also helps in speeding things up. \n",
    "\n",
    "## Output Size\n",
    "\n",
    "When a convolution is processed, the output size $S_c$ of the convolved can be determined from the following:\n",
    "\n",
    "$\\displaystyle S_c = \\frac{S_f - S_g + 2 P}{S} + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution, Padding, Strides in Higher Dimensions\n",
    "\n",
    "The explanation given in previous sections applies to convolution in 1D. In the case of images or matrices, the same logic can be extended to y-axis to get Convolution in 2D. In the case of 2D convolution, we would have:\n",
    "\n",
    "$\\displaystyle C(i, j) = \\sum_m \\sum_n f (i + m, j + n) g (m, n)$\n",
    "\n",
    "As an example, consider a 3×3 image $f$ and a 2×2 kernel $g$:\n",
    "\n",
    "$f =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$g =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "The convolution outputs would be:\n",
    "\n",
    "$C(0,0) = (1 \\times 1) + (2 \\times 0) + (4 \\times 0) + (5 \\times -1) = 1 + 0 + 0 - 5 = -4$\n",
    "\n",
    "$C(1,0) = (2 \\times 1) + (3 \\times 0) + (5 \\times 0) + (6 \\times -1) = 2 + 0 + 0 - 6 = -4$\n",
    "\n",
    "$C(0,1) = (4 \\times 1) + (5 \\times 0) + (7 \\times 0) + (8 \\times -1) = 4 + 0 + 0 - 8 = -4$\n",
    "\n",
    "$C(1,1) = (5 \\times 1) + (6 \\times 0) + (8 \\times 0) + (9 \\times -1) = 5 + 0 + 0 - 9 = -4$\n",
    "\n",
    "Likewise, padding and striding would also be applied along both x and y axis. \n",
    "\n",
    "The output size would then be:\n",
    "\n",
    "$\\displaystyle (S_c^x, S_c^y)= \\left(\\frac{S_f^x - S_g^x + 2 P^x}{S^x} + 1, \\frac{S_f^y - S_g^y + 2 P^y}{S^y} + 1\\right)$\n",
    "\n",
    "In the following, some code to help calculate these are provided:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "Kernels (or filters) are small matrices that slide over an image to perform feature extraction in image processing and deep learning. Different kernels emphasize different properties like edges, blurring, or sharpening.\n",
    "\n",
    "As an example, the following will leave an image unchanged:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "For identifying vertical edges, we may use $K_v$ and $K_h$ (Prewitt filters):\n",
    "\n",
    "$K_v = \\begin{bmatrix}\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1\n",
    "\\end{bmatrix}, K_h = \\begin{bmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    "0 & 0 & 0\\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "To do the same activity, with more emphasis on central pixels, a variation (Sobel filters) can be used:\n",
    "\n",
    "$K_v = \n",
    "\\begin{bmatrix}\n",
    "-1 & 0 & 1 \\\\\n",
    "-2 & 0 & 2 \\\\\n",
    "-1 & 0 & 1\n",
    "\\end{bmatrix}, K_h = \n",
    "\\begin{bmatrix}\n",
    "-1 & -2 & -1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "For both directions, we may use the Laplacian filters:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "1 & -4 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}, \\text{or} \\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & -8 & 1 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "For sharpening an image, we may have:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "0 & -1 & 0 \\\\\n",
    "-1 & 5 & -1 \\\\\n",
    "0 & -1 & 0\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "For blurring an image, we may have:\n",
    "\n",
    "$\\frac{1}{9}\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "or the following as a Gaussian blur, which will have a more smoothening effect:\n",
    "\n",
    "$\\frac{1}{16}\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 1 \\\\\n",
    "2 & 4 & 2 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "There are several other examples which will be mostly covered in a course on digital image processing. In the case of Convolution Neural Networks, we can start with a random kernel and the idea would be that the learning phase of the architecture will come up with any type of kernel. Some kernels may not even have a name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architectures\n",
    "\n",
    "A typical CNN consists of the following layers:\n",
    "\n",
    "## Convolution Layer\n",
    "\n",
    "This is the core building block of a CNN. It applies filters (kernels) to extract meaningful features such as edges, textures, and patterns. Each filter slides over the image, performing element-wise multiplication and summing the results (convolution operation). The result is called a feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conv2d(image, kernel):\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "    image_height, image_width = image.shape\n",
    "    output_height = image_height - kernel_height + 1\n",
    "    output_width = image_width - kernel_width + 1\n",
    "    output = np.zeros((output_height, output_width))\n",
    "    \n",
    "    for i in range(output_height):\n",
    "        for j in range(output_width):\n",
    "            region = image[i:i+kernel_height, j:j+kernel_width]\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After convolution layer, a ReLU (Rectified Linear Unit) activation function is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer\n",
    "\n",
    "Pooling layers reduce the spatial dimensions while retaining important information. The most common types are **Max Pooling**, which takes the maximum value from each region, and **Average Pooling**, which takes the average value from the same region. Pooling helps in reducing the computation as well as controls overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(image, pool_size=2):\n",
    "    image_height, image_width = image.shape\n",
    "\n",
    "    output_height = (image_height + pool_size - 1) // pool_size\n",
    "    output_width = (image_width + pool_size - 1) // pool_size\n",
    "    output = np.zeros((output_height, output_width))\n",
    "\n",
    "    for i in range(output_height):\n",
    "        for j in range(output_width):\n",
    "            start_i = i * pool_size\n",
    "            start_j = j * pool_size\n",
    "            end_i = min(start_i + pool_size, image_height)\n",
    "            end_j = min(start_j + pool_size, image_width)\n",
    "\n",
    "            region = image[start_i:end_i, start_j:end_j]\n",
    "            output[i, j] = np.max(region)\n",
    "\n",
    "    return output\n",
    "\n",
    "def avg_pooling(image, pool_size=2):\n",
    "    image_height, image_width = image.shape\n",
    "\n",
    "    output_height = (image_height + pool_size - 1) // pool_size\n",
    "    output_width = (image_width + pool_size - 1) // pool_size\n",
    "    output = np.zeros((output_height, output_width))\n",
    "\n",
    "    for i in range(output_height):\n",
    "        for j in range(output_width):\n",
    "            start_i = i * pool_size\n",
    "            start_j = j * pool_size\n",
    "            end_i = min(start_i + pool_size, image_height)\n",
    "            end_j = min(start_j + pool_size, image_width)\n",
    "\n",
    "            region = image[start_i:end_i, start_j:end_j]\n",
    "            output[i, j] = np.average(region)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Dense Layer\n",
    "\n",
    "After feature extraction, the output is flattened and passed through fully connected layers, where each neuron is connected to every neuron in the next layer. The output of this layer is the application of the softmax activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(inputs, weights, bias):\n",
    "    return np.dot(inputs, weights) + bias\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Example\n",
    "\n",
    "## Input Data\n",
    "\n",
    "We start with an example 5x5 image representing some numbers given in binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array([\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [0, 1, 1, 1, 0]\n",
    "])\n",
    "\n",
    "kernel = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [1, 0, 1]\n",
    "]).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Output:\n",
      " [[1. 2. 1.]\n",
      " [0. 2. 0.]\n",
      " [1. 2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "conv_output = conv2d(image, kernel)\n",
    "print(\"Convolution Output:\\n\", conv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ReLU Output:\n",
      " [[1. 2. 1.]\n",
      " [0. 2. 0.]\n",
      " [1. 2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "relu_output = relu(conv_output)\n",
    "print(\"\\nReLU Output:\\n\", relu_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Max Pooling Output:\n",
      " [[2. 1.]\n",
      " [2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "pool_output = max_pooling(relu_output)\n",
    "print(\"\\nMax Pooling Output:\\n\", pool_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fully Connected Layer Output:\n",
      " [ 0.05802171 -0.00893779  0.06423701 -0.05868947 -0.01187308  0.00291787\n",
      " -0.03199532  0.0389161   0.01085836  0.00250945]\n"
     ]
    }
   ],
   "source": [
    "flattened = pool_output.flatten()\n",
    "\n",
    "fc_weights = np.random.randn(len(flattened), 10)*0.01\n",
    "fc_bias = np.random.randn(10)*0.01\n",
    "\n",
    "dense_output = dense_layer(flattened, fc_weights, fc_bias)\n",
    "print(\"\\nFully Connected Layer Output:\\n\", dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Softmax Output (Probabilities):\n",
      " [0.10520655 0.09839264 0.10586247 0.09361722 0.09810425 0.09956609\n",
      " 0.0961499  0.10321559 0.10035984 0.09952544]\n"
     ]
    }
   ],
   "source": [
    "softmax_output = softmax(dense_output)\n",
    "print(\"\\nSoftmax Output (Probabilities):\\n\", softmax_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher value of the softmax will mean that an input image belongs to that particular class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Feature Learning\n",
    "Lets assume that the input image provided belongs to either labels 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. For this, we create a one-hot encoding target class for 8 (if 8 is provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]\n",
      " [0 1 0 1 0]\n",
      " [0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(image)\n",
    "label = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.2946\n",
      "Epoch 2, Loss: 2.2702\n",
      "Epoch 3, Loss: 2.2460\n",
      "Epoch 4, Loss: 2.2216\n",
      "Epoch 5, Loss: 2.1972\n",
      "Epoch 6, Loss: 2.1726\n",
      "Epoch 7, Loss: 2.1478\n",
      "Epoch 8, Loss: 2.1226\n",
      "Epoch 9, Loss: 2.0969\n",
      "Epoch 10, Loss: 2.0708\n",
      "Epoch 11, Loss: 2.0441\n",
      "Epoch 12, Loss: 2.0168\n",
      "Epoch 13, Loss: 1.9888\n",
      "Epoch 14, Loss: 1.9600\n",
      "Epoch 15, Loss: 1.9304\n",
      "Epoch 16, Loss: 1.8998\n",
      "Epoch 17, Loss: 1.8683\n",
      "Epoch 18, Loss: 1.8357\n",
      "Epoch 19, Loss: 1.8020\n",
      "Epoch 20, Loss: 1.7672\n",
      "Epoch 21, Loss: 1.7313\n",
      "Epoch 22, Loss: 1.6940\n",
      "Epoch 23, Loss: 1.6556\n",
      "Epoch 24, Loss: 1.6158\n",
      "Epoch 25, Loss: 1.5748\n",
      "Epoch 26, Loss: 1.5325\n",
      "Epoch 27, Loss: 1.4890\n",
      "Epoch 28, Loss: 1.4442\n",
      "Epoch 29, Loss: 1.3983\n",
      "Epoch 30, Loss: 1.3514\n",
      "Epoch 31, Loss: 1.3035\n",
      "Epoch 32, Loss: 1.2548\n",
      "Epoch 33, Loss: 1.2054\n",
      "Epoch 34, Loss: 1.1555\n",
      "Epoch 35, Loss: 1.1054\n",
      "Epoch 36, Loss: 1.0552\n",
      "Epoch 37, Loss: 1.0052\n",
      "Epoch 38, Loss: 0.9555\n",
      "Epoch 39, Loss: 0.9066\n",
      "Epoch 40, Loss: 0.8585\n",
      "Epoch 41, Loss: 0.8116\n",
      "Epoch 42, Loss: 0.7660\n",
      "Epoch 43, Loss: 0.7220\n",
      "Epoch 44, Loss: 0.6797\n",
      "Epoch 45, Loss: 0.6393\n",
      "Epoch 46, Loss: 0.6008\n",
      "Epoch 47, Loss: 0.5644\n",
      "Epoch 48, Loss: 0.5301\n",
      "Epoch 49, Loss: 0.4979\n",
      "Epoch 50, Loss: 0.4677\n",
      "Updated Kernel: [[-0.29453901  0.02314205  0.27612342]\n",
      " [ 0.10761338 -0.38281338  0.10761338]\n",
      " [ 0.70546099  0.02314205  1.27612342]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.04311831, 0.04190594, 0.04260944, 0.03982661, 0.04103404,\n",
       "       0.04115852, 0.04008135, 0.04218675, 0.62644565, 0.0416334 ])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.01\n",
    "for epoch in range(epochs):\n",
    "    # Forward\n",
    "    feature_map  = conv2d(image, kernel)\n",
    "    activated    = relu(feature_map)\n",
    "    pooled       = max_pooling(activated, pool_size=2)\n",
    "    flattened    = pooled.flatten()\n",
    "    dense_output = dense_layer(flattened, fc_weights, fc_bias)\n",
    "    output_layer = softmax(dense_output)\n",
    "\n",
    "    loss_gradient = output_layer - label\n",
    "        \n",
    "    #fc_weights_gradient = np.outer(flattened, loss_gradient)\n",
    "    fc_weights_gradient = np.dot(flattened[:, np.newaxis], loss_gradient[np.newaxis, :])\n",
    "    fc_bias_gradient    = loss_gradient\n",
    "    flattened_gradient  = np.dot(fc_weights, loss_gradient)\n",
    "\n",
    "    # Reshape for pooling layer\n",
    "    pooled_gradient = flattened_gradient.reshape(pooled.shape)\n",
    "    \n",
    "    # Backprop through max pooling\n",
    "    feature_map_gradient = np.zeros_like(activated)\n",
    "    for i in range(pooled.shape[0]):\n",
    "        for j in range(pooled.shape[1]):\n",
    "            max_val = pooled[i, j]\n",
    "            for m in range(2):\n",
    "                for n in range(2):\n",
    "                    orig_x, orig_y = i*2 + m, j*2 + n\n",
    "                    if orig_x < activated.shape[0] and orig_y < activated.shape[1]:  # Check bounds\n",
    "                        if activated[orig_x, orig_y] == max_val:\n",
    "                            feature_map_gradient[orig_x, orig_y] = pooled_gradient[i, j]\n",
    "\n",
    "    \n",
    "    # Backprop through ReLU\n",
    "    feature_map_gradient *= np.where(feature_map > 0, 1, 0)\n",
    "    \n",
    "    # Backprop through convolution\n",
    "    kernel_gradient = np.zeros_like(kernel)\n",
    "    for i in range(kernel.shape[0]):\n",
    "        for j in range(kernel.shape[1]):\n",
    "            kernel_gradient[i, j] = np.sum(image[i:i+feature_map.shape[0], j:j+feature_map.shape[1]] * feature_map_gradient)\n",
    "    \n",
    "    # Update weights\n",
    "    fc_weights -= learning_rate * fc_weights_gradient\n",
    "    fc_bias -= learning_rate * fc_bias_gradient\n",
    "    kernel -= learning_rate * kernel_gradient\n",
    "\n",
    "    loss = -np.sum(label * np.log(output_layer + 1e-9))\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"Updated Kernel:\", kernel)\n",
    "output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending to Multiple Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    np.array([\n",
    "        [0, 1, 1, 1, 0],\n",
    "        [0, 1, 0, 1, 0],\n",
    "        [0, 1, 1, 1, 0],\n",
    "        [0, 1, 0, 1, 0],\n",
    "        [0, 1, 1, 1, 0]\n",
    "    ]),\n",
    "    np.array([\n",
    "        [0, 1, 1, 1, 0],\n",
    "        [0, 0, 0, 1, 0],\n",
    "        [0, 0, 1, 0, 0],\n",
    "        [0, 1, 0, 0, 0],\n",
    "        [0, 1, 1, 1, 0]\n",
    "    ])\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0]),  # Label for first image\n",
    "    np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),  # Label for second image\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4218\n",
      "Epoch 2, Loss: 0.3993\n",
      "Epoch 3, Loss: 0.3850\n",
      "Epoch 4, Loss: 0.3749\n",
      "Epoch 5, Loss: 0.3673\n",
      "Epoch 6, Loss: 0.3609\n",
      "Epoch 7, Loss: 0.3554\n",
      "Epoch 8, Loss: 0.3503\n",
      "Epoch 9, Loss: 0.3455\n",
      "Epoch 10, Loss: 0.3410\n",
      "Epoch 11, Loss: 0.3366\n",
      "Epoch 12, Loss: 0.3323\n",
      "Epoch 13, Loss: 0.3281\n",
      "Epoch 14, Loss: 0.3240\n",
      "Epoch 15, Loss: 0.3199\n",
      "Epoch 16, Loss: 0.3159\n",
      "Epoch 17, Loss: 0.3119\n",
      "Epoch 18, Loss: 0.3080\n",
      "Epoch 19, Loss: 0.3041\n",
      "Epoch 20, Loss: 0.3003\n",
      "Epoch 21, Loss: 0.2965\n",
      "Epoch 22, Loss: 0.2927\n",
      "Epoch 23, Loss: 0.2890\n",
      "Epoch 24, Loss: 0.2853\n",
      "Epoch 25, Loss: 0.2816\n",
      "Epoch 26, Loss: 0.2780\n",
      "Epoch 27, Loss: 0.2745\n",
      "Epoch 28, Loss: 0.2709\n",
      "Epoch 29, Loss: 0.2674\n",
      "Epoch 30, Loss: 0.2640\n",
      "Epoch 31, Loss: 0.2606\n",
      "Epoch 32, Loss: 0.2572\n",
      "Epoch 33, Loss: 0.2539\n",
      "Epoch 34, Loss: 0.2506\n",
      "Epoch 35, Loss: 0.2474\n",
      "Epoch 36, Loss: 0.2442\n",
      "Epoch 37, Loss: 0.2410\n",
      "Epoch 38, Loss: 0.2379\n",
      "Epoch 39, Loss: 0.2348\n",
      "Epoch 40, Loss: 0.2317\n",
      "Epoch 41, Loss: 0.2287\n",
      "Epoch 42, Loss: 0.2258\n",
      "Epoch 43, Loss: 0.2229\n",
      "Epoch 44, Loss: 0.2200\n",
      "Epoch 45, Loss: 0.2171\n",
      "Epoch 46, Loss: 0.2143\n",
      "Epoch 47, Loss: 0.2116\n",
      "Epoch 48, Loss: 0.2088\n",
      "Epoch 49, Loss: 0.2062\n",
      "Epoch 50, Loss: 0.2035\n",
      "Updated Kernel: [[ 0.30712358  0.2989825   0.41009054]\n",
      " [ 0.5870249  -0.57657332  0.57721377]\n",
      " [ 1.30712358  0.2989825   1.41009054]]\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.01\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for img, label in zip(dataset, labels):\n",
    "        \n",
    "        # Forward\n",
    "        feature_map  = conv2d(img, kernel)\n",
    "        activated    = relu(feature_map)\n",
    "        pooled       = max_pooling(activated, pool_size=2)\n",
    "        flattened    = pooled.flatten()\n",
    "        dense_output = dense_layer(flattened, fc_weights, fc_bias)\n",
    "        output_layer = softmax(dense_output)\n",
    "\n",
    "        loss_gradient = output_layer - label\n",
    "        \n",
    "        #fc_weights_gradient = np.outer(flattened, loss_gradient)\n",
    "        fc_weights_gradient = np.dot(flattened[:, np.newaxis], loss_gradient[np.newaxis, :])\n",
    "        fc_bias_gradient    = loss_gradient\n",
    "        flattened_gradient  = np.dot(fc_weights, loss_gradient)\n",
    "\n",
    "        # Reshape for pooling layer\n",
    "        pooled_gradient = flattened_gradient.reshape(pooled.shape)\n",
    "    \n",
    "        # Backprop through max pooling\n",
    "        feature_map_gradient = np.zeros_like(activated)\n",
    "        for i in range(pooled.shape[0]):\n",
    "            for j in range(pooled.shape[1]):\n",
    "                max_val = pooled[i, j]\n",
    "                for m in range(2):\n",
    "                    for n in range(2):\n",
    "                        orig_x, orig_y = i*2 + m, j*2 + n\n",
    "                        if orig_x < activated.shape[0] and orig_y < activated.shape[1]:  # Check bounds\n",
    "                            if activated[orig_x, orig_y] == max_val:\n",
    "                                feature_map_gradient[orig_x, orig_y] = pooled_gradient[i, j]\n",
    "\n",
    "    \n",
    "        # Backprop through ReLU\n",
    "        feature_map_gradient *= np.where(feature_map > 0, 1, 0)\n",
    "    \n",
    "        # Backprop through convolution\n",
    "        kernel_gradient = np.zeros_like(kernel)\n",
    "        for i in range(kernel.shape[0]):\n",
    "            for j in range(kernel.shape[1]):\n",
    "                kernel_gradient[i, j] = np.sum(image[i:i+feature_map.shape[0], j:j+feature_map.shape[1]] * feature_map_gradient)\n",
    "    \n",
    "        # Update weights\n",
    "        fc_weights -= learning_rate * fc_weights_gradient\n",
    "        fc_bias -= learning_rate * fc_bias_gradient\n",
    "        kernel -= learning_rate * kernel_gradient\n",
    "\n",
    "        loss = -np.sum(label * np.log(output_layer + 1e-9))\n",
    "        total_loss += loss\n",
    "    \n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Updated Kernel:\", kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0021531 , 0.00204774, 0.77721847, 0.00196863, 0.00206441,\n",
       "       0.00208411, 0.00199064, 0.00218223, 0.20627317, 0.0020175 ])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.array([\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 1, 0],\n",
    "        [1, 0, 1, 0, 1],\n",
    "        [0, 1, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1]\n",
    "    ])\n",
    "\n",
    "true_label = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 2\n",
      "True Class: 2\n",
      "Output Probabilities: [5.16254350e-04 4.43114414e-04 7.88553716e-01 4.31629893e-04\n",
      " 4.61064763e-04 4.84853241e-04 4.54236329e-04 5.16126405e-04\n",
      " 2.07675228e-01 4.63776768e-04]\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass for Testing\n",
    "feature_map  = conv2d(test_image, kernel)\n",
    "activated    = relu(feature_map)\n",
    "pooled       = max_pooling(activated, pool_size=2)\n",
    "flattened    = pooled.flatten()\n",
    "dense_output = dense_layer(flattened, fc_weights, fc_bias)\n",
    "output_layer = softmax(dense_output)\n",
    "\n",
    "# Predicted class\n",
    "predicted_label = np.argmax(output_layer)\n",
    "true_class = np.argmax(true_label)\n",
    "\n",
    "print(\"Predicted Class:\", predicted_label)\n",
    "print(\"True Class:\", true_class)\n",
    "print(\"Output Probabilities:\", output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Label: [0 0 1 0 0 0 0 0 0 0]\n",
      "Predicted Probabilities: [5.16254350e-04 4.43114414e-04 7.88553716e-01 4.31629893e-04\n",
      " 4.61064763e-04 4.84853241e-04 4.54236329e-04 5.16126405e-04\n",
      " 2.07675228e-01 4.63776768e-04]\n",
      "Argmax of True Label: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected Label:\", true_label)\n",
    "print(\"Predicted Probabilities:\", output_layer)\n",
    "print(\"Argmax of True Label:\", np.argmax(true_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 655490,
     "sourceId": 1158529,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 658845,
     "sourceId": 1163269,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
